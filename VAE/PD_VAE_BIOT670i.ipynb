{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORfPo0X0B3OQ9l/XnTevQu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4fe3bb4fc97e4f7e93f111952cb0c69e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Filename:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_dd018b4d83b74e2b964bb7923eef938a",
            "placeholder": "​",
            "style": "IPY_MODEL_c60616561d834385916e1b513e9467cd",
            "value": "my_molecules.png"
          }
        },
        "dd018b4d83b74e2b964bb7923eef938a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60616561d834385916e1b513e9467cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70310d53d8f540529ac14d402f712efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Save Image",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ef4003feb9e5472f9d7ca73ea708672c",
            "style": "IPY_MODEL_71806e09eb2247c4a3f687cf902ad683",
            "tooltip": ""
          }
        },
        "ef4003feb9e5472f9d7ca73ea708672c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71806e09eb2247c4a3f687cf902ad683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ef76a39566354d4ebe25095fdf2157ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "CSV Filename:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_383d5dbf3e4149d0b51d78ee2c7d5ea5",
            "placeholder": "​",
            "style": "IPY_MODEL_11772c1b11b848c5a17aafdba989b56a",
            "value": "generated_molecules.csv"
          }
        },
        "383d5dbf3e4149d0b51d78ee2c7d5ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11772c1b11b848c5a17aafdba989b56a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1687d823ca34c58966b5264a20677de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Save CSV",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7028a3e101e64cd7a451ec1a5e2984a7",
            "style": "IPY_MODEL_cf2a6c01d9584faba7d3d1968d6d8678",
            "tooltip": ""
          }
        },
        "7028a3e101e64cd7a451ec1a5e2984a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf2a6c01d9584faba7d3d1968d6d8678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AKShaffer/Group-2---Generative-AI-Tools-for-Drug-Repurposing-for-PD/blob/main/PD_VAE_BIOT670i.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is adapted from: https://keras.io/examples/generative/molecule_generation/"
      ],
      "metadata": {
        "id": "p3FLEeu89C3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Novel SMILES generation using VAE and Parkinson's Disease market drugs as training data\n",
        "\n",
        "**Author:** Angela Shaffer<br>\n",
        "**Date created:** 2025/03/14<br>\n",
        "**Last modified:** 2025/03/14<br>"
      ],
      "metadata": {
        "id": "_Qjy7cFeCiy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook implements a Variational Autoencoder (VAE) to generate SMILES strings for Parkinson's Disease (PD) drug discovery and potential drug repositioning if any of the novel molecules are similar to existing market approved drugs not currently approved for PD treatment.\n",
        "\n",
        "This code is an adaptation of [**Drug Molecule Generation with VAE**](https://keras.io/examples/generative/molecule_generation/) by Victor Basu. The majority of the functions used in this notebook are from this Keras paper. Additional functions are derived from [WGAN-GP with R-GCN for the generation of small molecular graphs](https://keras.io/examples/generative/wgan-graphs/)."
      ],
      "metadata": {
        "id": "q-vZHBdXCrG-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0Ne9mrJ9R7J"
      },
      "source": [
        "# Install packages and import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cf9aGoNh339C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4948f1e9-4202-478a-8954-b08a85392ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m715.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Install and import dependencies\n",
        "!pip -q install cairocffi\n",
        "!pip -q install rdkit-pypi\n",
        "!pip -q install Pillow\n",
        "!pip -q install rdkit\n",
        "\n",
        "'''\n",
        "os: For interacting with the operating system.\n",
        "io: For handling input and output operations.\n",
        "sys: For system-specific parameters and functions.\n",
        "google.colab, drive: Libraries specific to Google Colaboratory, used for mounting Google Drive to access files.\n",
        "cv2: OpenCV (cv2), for computer vision tasks.\n",
        "BytesIO: From the io module, for working with in-memory byte streams.\n",
        "csv: For reading and writing CSV (Comma-Separated Values) files.\n",
        "PIL, Image: Pillow (PIL) library for image manipulation.\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\": Sets the backend for Keras, a deep learning library, to TensorFlow.\n",
        "ast: For working with Abstract Syntax Trees.\n",
        "matplotlib, plt: Matplotlib for creating visualizations.\n",
        "ipywidgets, widgets: For creating interactive widgets in Jupyter notebooks.\n",
        "IPython, get_ipython, display: IPython functionalities for interactive computing and display in Jupyter notebooks.\n",
        "pandas, pd: Pandas for data manipulation and analysis using DataFrames.\n",
        "numpy, np: NumPy for numerical computing with arrays.\n",
        "sklearn, TruncatedSVD: Scikit-learn for machine learning tasks, here importing TruncatedSVD for dimensionality reduction.\n",
        "tensorflow, tf: TensorFlow for deep learning.\n",
        "keras, layers, ops: Keras for building and training neural networks.\n",
        "rdkit, Chem, RDLogger, etc.: RDKit, a cheminformatics library for working with chemical data, molecules, and their properties.\n",
        "sys.path.append(...): Adds the RDKit's Contrib directory to the Python path, making its modules accessible.\n",
        "sascorer: A module for calculating the Synthetic Accessibility Score (SAS) of molecules.\n",
        "RDLogger.DisableLog(\"rdApp.*\"): Disables specific RDKit logs to reduce output clutter.\n",
        "'''\n",
        "\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import cv2\n",
        "from io import BytesIO\n",
        "import csv\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import ast\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "\n",
        "\n",
        "import rdkit\n",
        "from rdkit import Chem, RDLogger\n",
        "from rdkit.Chem import AllChem, BondType, Crippen, Descriptors, Draw\n",
        "from rdkit.Chem import MolToSmiles, QED, RDConfig, rdmolops, rdMolDescriptors\n",
        "from rdkit.Chem.Draw import MolsToGridImage, rdMolDraw2D, IPythonConsole, rdDepictor\n",
        "\n",
        "# Add the RDKit's Contrib directory to the Python path\n",
        "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
        "\n",
        "import sascorer\n",
        "\n",
        "RDLogger.DisableLog(\"rdApp.*\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset is a curated list of market-approved drugs for treating Parkinson's Disease. The columns include the dataset include the molecule formula in SMILE representation along with their respective molecular properties such as **logP** (water–octanal partition coefficient), **SAS** (synthetic accessibility score) and **QED** (Qualitative Estimate of Drug-likeness).\n",
        "\n",
        "NOTE: At the end of this notebook is a section titled **Supplemental Code** that contains additional helper functions that can be implemented as needed. These functions include functions for calculating logP, QED, and SAS scores in a smiles dataset."
      ],
      "metadata": {
        "id": "5jKKPVa9-A2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "csv_path = '/content/drive/My Drive/VAE_GAN_Colab_Notebooks/Data/Full_Reference_Database_VAE_no repeats.csv'\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# The function find_max_atoms iterates through each SMILES string in the 'smiles' column of the\n",
        "# DataFrame. Each SMILES string is then converted into a molecule object (mol) using\n",
        "# Chem.MolFromSmiles. The number of atoms in the molecule (num_atoms) is then obtained using\n",
        "# mol.GetNumAtoms. The function then updates max_atoms to store the largest number of atoms\n",
        "# encountered so far. Finally, it returns max_atoms, representing the maximum number of atoms in\n",
        "# any molecule within the dataset.\n",
        "def find_max_atoms(df):\n",
        "\n",
        "    max_atoms = 0\n",
        "    for smiles in df['smiles']:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        num_atoms = mol.GetNumAtoms()\n",
        "        max_atoms = max(max_atoms, num_atoms)\n",
        "    return max_atoms\n",
        "\n",
        "# Find the highest number of atoms from a single SMILES data point\n",
        "# from the dataset and display the result\n",
        "result = find_max_atoms(df)\n",
        "print(\"Maximum number of atoms:\", result)\n",
        "\n",
        "# Cleans the dataframe by removing newline characters from the smiles column\n",
        "df[\"smiles\"] = df[\"smiles\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n",
        "\n",
        "\n",
        "# Displays the first few rows of the DataFrame, giving a glimpse of the data.\n",
        "display(df.head())\n",
        "# Takes the SMILES string from the 6th row (index 5) of the DataFrame and converts it into a\n",
        "# molecule object, then displays an image and the smiles of the molecule object.\n",
        "molecule = Chem.MolFromSmiles(df[\"smiles\"][5])\n",
        "display(Draw.MolToImage(molecule))\n",
        "print(df[\"smiles\"][5])"
      ],
      "metadata": {
        "id": "y9NILfcu7w8h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "01667c57-3d9c-4042-cbd6-cdff31e803e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum number of atoms: 30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              smiles    logP       qed   sas\n",
              "0                              C1C2CC3CC1CC(C2)(C3)N  1.9139  0.562066  0.30\n",
              "1                           C1C2CC3CC1CC(C2)(C3)N.Cl  2.3357  0.619244  0.38\n",
              "2    CN1CCC2=C3[C@H]1CC4=C(C3=CC=C2)C(=C(C=C4)O)O.Cl  3.2717  0.734189  0.61\n",
              "3  CN1[C@@H]2CC[C@H]1CC(C2)OC(C3=CC=CC=C3)C4=CC=C...  3.9218  0.783948  1.21\n",
              "4  CC(CC1=CC(=C(C=C1)O)O)(C(=O)O)NN.C1=CC(=C(C=C1... -0.0009  0.163746  1.55"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3492afc8-3178-446d-a05c-2e8de2701eae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>smiles</th>\n",
              "      <th>logP</th>\n",
              "      <th>qed</th>\n",
              "      <th>sas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C1C2CC3CC1CC(C2)(C3)N</td>\n",
              "      <td>1.9139</td>\n",
              "      <td>0.562066</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C1C2CC3CC1CC(C2)(C3)N.Cl</td>\n",
              "      <td>2.3357</td>\n",
              "      <td>0.619244</td>\n",
              "      <td>0.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CN1CCC2=C3[C@H]1CC4=C(C3=CC=C2)C(=C(C=C4)O)O.Cl</td>\n",
              "      <td>3.2717</td>\n",
              "      <td>0.734189</td>\n",
              "      <td>0.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CN1[C@@H]2CC[C@H]1CC(C2)OC(C3=CC=CC=C3)C4=CC=C...</td>\n",
              "      <td>3.9218</td>\n",
              "      <td>0.783948</td>\n",
              "      <td>1.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CC(CC1=CC(=C(C=C1)O)O)(C(=O)O)NN.C1=CC(=C(C=C1...</td>\n",
              "      <td>-0.0009</td>\n",
              "      <td>0.163746</td>\n",
              "      <td>1.55</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3492afc8-3178-446d-a05c-2e8de2701eae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3492afc8-3178-446d-a05c-2e8de2701eae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3492afc8-3178-446d-a05c-2e8de2701eae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1401996d-b0c6-4c9e-9209-631591cdb691\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1401996d-b0c6-4c9e-9209-631591cdb691')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1401996d-b0c6-4c9e-9209-631591cdb691 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(df[\\\"smiles\\\"][5])\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"smiles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"C1C2CC3CC1CC(C2)(C3)N.Cl\",\n          \"CC(CC1=CC(=C(C=C1)O)O)(C(=O)O)NN.C1=CC(=C(C=C1C[C@@H](C(=O)O)N)O)O\",\n          \"CN1CCC2=C3[C@H]1CC4=C(C3=CC=C2)C(=C(C=C4)O)O.Cl\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"logP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5015206019232636,\n        \"min\": -0.0009,\n        \"max\": 3.9218,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2.3357,\n          -0.0009,\n          3.2717\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"qed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24506482113392195,\n        \"min\": 0.163745998,\n        \"max\": 0.783947816,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.619243881,\n          0.163745998,\n          0.734188896\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sas\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5460311346434378,\n        \"min\": 0.3,\n        \"max\": 1.55,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.38,\n          1.55,\n          0.61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CCN(CC)C(=O)/C(=C/C1=CC(=C(C(=C1)O)O)[N+](=O)[O-])/C#N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Hyperparameters"
      ],
      "metadata": {
        "id": "92aOik3_-kuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract unique atom types from SMILES\n",
        "def get_unique_atoms(smiles_list):\n",
        "    unique_atoms = set()\n",
        "    for s in smiles_list:\n",
        "        mol = Chem.MolFromSmiles(s)\n",
        "        if mol:\n",
        "            for atom in mol.GetAtoms():\n",
        "                unique_atoms.add(atom.GetSymbol())\n",
        "    return unique_atoms\n",
        "\n",
        "# Map each unique atom type to a dictionary\n",
        "def map_atoms_to_dict(unique_atoms):\n",
        "    atom_dict = {}\n",
        "    for idx, atom in enumerate(sorted(unique_atoms)):\n",
        "        atom_dict[atom] = idx\n",
        "    return atom_dict\n",
        "\n",
        "SMILE_CHARSET = get_unique_atoms(df[\"smiles\"])\n",
        "\n",
        "# bond_mapping: This dictionary maps bond types (e.g., 'SINGLE', 'DOUBLE') to numerical indices\n",
        "# and vice-versa. This is similar to the atom mapping and is used for representing bonds in the\n",
        "# model.\n",
        "bond_mapping = {\"SINGLE\": 0, \"DOUBLE\": 1, \"TRIPLE\": 2, \"AROMATIC\": 3}\n",
        "bond_mapping.update(\n",
        "    {0: BondType.SINGLE, 1: BondType.DOUBLE, 2: BondType.TRIPLE, 3: BondType.AROMATIC}\n",
        ")\n",
        "\n",
        "'''\n",
        "MAX_MOLSIZE, MIN_MOLSIZE: These variables store the maximum and minimum lengths of the SMILES\n",
        "strings in the dataset, respectively. This gives an idea of the size range of the molecules.\n",
        "SMILE_to_index, index_to_SMILE: These dictionaries provide mappings between the unique\n",
        "characters in the SMILES strings and numerical indices, similar to what we had for atoms and\n",
        "bonds.\n",
        "atom_mapping: A combination of the SMILE_to_index and index_to_SMILE mappings, for convenient\n",
        "access to atom-to-index and index-to-atom conversions.\n",
        "BATCH_SIZE: This hyperparameter determines how many samples are processed together during\n",
        "training.\n",
        "VAE_LR: Learning rate for the VAE optimizer, a parameter that controls the step size during\n",
        "model optimization.\n",
        "NUM_ATOMS: Stores the maximum number of atoms in any molecule in the dataset (result), which\n",
        "helps define the input size for the model.\n",
        "ATOM_DIM: Number of unique atom types in the dataset, defining the size of the feature vectors\n",
        "representing atoms.\n",
        "BOND_DIM: Number of bond types (including \"non-bond\"), which is used to represent the\n",
        "connectivity between atoms.\n",
        "LATENT_DIM: The size of the latent space in the VAE, representing the compressed representation\n",
        " of the molecules.\n",
        "'''\n",
        "MAX_MOLSIZE = max(df[\"smiles\"].str.len())\n",
        "MIN_MOLSIZE = min(df[\"smiles\"].str.len())\n",
        "SMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))\n",
        "index_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))\n",
        "atom_mapping = dict(SMILE_to_index)\n",
        "atom_mapping.update(index_to_SMILE)\n",
        "print(atom_mapping)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "VAE_LR = 5e-4\n",
        "NUM_ATOMS = result  # Maximum number of atoms\n",
        "\n",
        "ATOM_DIM = len(SMILE_CHARSET)  # Number of atom types\n",
        "BOND_DIM = 4 + 1  # Number of bond types\n",
        "LATENT_DIM = 64  # Size of the latent space\n",
        "\n",
        "\n",
        "# smiles_to_graph: This function takes a SMILES string as input and converts it into a graph\n",
        "# representation consisting of two tensors:\n",
        "  # adjacency: A tensor representing the bonds between atoms in the molecule.\n",
        "  # features: A tensor representing the features of each atom in the molecule (e.g., atom type).\n",
        "def smiles_to_graph(smiles):\n",
        "    # Converts SMILES to molecule object\n",
        "    molecule = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    # Initialize adjacency and feature tensor\n",
        "    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), \"float32\")\n",
        "    features = np.zeros((NUM_ATOMS, ATOM_DIM), \"float32\")\n",
        "\n",
        "    # loop over each atom in molecule\n",
        "    for atom in molecule.GetAtoms():\n",
        "        i = atom.GetIdx()\n",
        "        atom_type = atom_mapping[atom.GetSymbol()]\n",
        "        features[i] = np.eye(ATOM_DIM)[atom_type]\n",
        "        # loop over one-hop neighbors\n",
        "        for neighbor in atom.GetNeighbors():\n",
        "            j = neighbor.GetIdx()\n",
        "            bond = molecule.GetBondBetweenAtoms(i, j)\n",
        "            bond_type_idx = bond_mapping[bond.GetBondType().name]\n",
        "            adjacency[bond_type_idx, [i, j], [j, i]] = 1\n",
        "\n",
        "    # Where no bond, add 1 to last channel (indicating \"non-bond\")\n",
        "    # Notice: channels-first\n",
        "    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1\n",
        "\n",
        "    # Where no atom, add 1 to last column (indicating \"non-atom\")\n",
        "    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1\n",
        "\n",
        "    return adjacency, features\n",
        "\n",
        "# graph_to_molecule: This function takes the graph representation (adjacency and features) and\n",
        "# converts it back into a molecule object\n",
        "def graph_to_molecule(graph):\n",
        "    # Unpack graph\n",
        "    adjacency, features = graph\n",
        "\n",
        "    # RWMol is a molecule object intended to be edited\n",
        "    molecule = Chem.RWMol()\n",
        "\n",
        "    # Remove \"no atoms\" & atoms with no bonds\n",
        "    keep_idx = np.where(\n",
        "        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n",
        "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n",
        "    )[0]\n",
        "    features = features[keep_idx]\n",
        "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
        "\n",
        "    # Add atoms to molecule\n",
        "    for atom_type_idx in np.argmax(features, axis=1):\n",
        "        atom = Chem.Atom(atom_mapping[atom_type_idx])\n",
        "        _ = molecule.AddAtom(atom)\n",
        "\n",
        "    # Add bonds between atoms in molecule; based on the upper triangles of the [symmetric]\n",
        "    # adjacency tensor\n",
        "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
        "    for bond_ij, atom_i, atom_j in zip(bonds_ij, atoms_i, atoms_j):\n",
        "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
        "            continue\n",
        "        bond_type = bond_mapping[bond_ij]\n",
        "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
        "\n",
        "    # Sanitize the molecule; This function removes invalid smiles. For more information on\n",
        "    # sanitization, see https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization\n",
        "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
        "    # Strict sanitization rules implemented i.e., if sanitization fails, return None\n",
        "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
        "        return None\n",
        "\n",
        "    return molecule"
      ],
      "metadata": {
        "id": "ps60Ouk68QQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4dc2184-6820-4713-bb82-f26d81811d86"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'S': 0, 'C': 1, 'F': 2, 'O': 3, 'Cl': 4, 'N': 5, 0: 'S', 1: 'C', 2: 'F', 3: 'O', 4: 'Cl', 5: 'N'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare and transform training dataset"
      ],
      "metadata": {
        "id": "-6PG7XAd-v3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df = df.sample(frac=1, random_state=42): This line shuffles the rows of the DataFrame df\n",
        "# to randomize the data for training. frac=1 means it shuffles all rows. random_state=42 ensures\n",
        "# that the shuffling is reproducible, meaning you'll get the same order every time you run the\n",
        "# code with this seed.\n",
        "train_df = df.sample(frac=1, random_state=42)  # random state is a seed value\n",
        "\n",
        "# train_df.reset_index(drop=True, inplace=True): After shuffling, the index of the DataFrame is\n",
        "# reset to sequential numbers (0, 1, 2, etc.), and the old index is dropped using drop=True.\n",
        "# inplace=True modifies the DataFrame directly.\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# row_count = len(train_df): This gets the total number of rows (data points) in the shuffled\n",
        "# DataFrame.\n",
        "row_count = len(train_df)  # Use this for training sets less than 8000\n",
        "\n",
        "# adjacency_tensor, feature_tensor, qed_tensor = [], [], []: This initializes three empty lists\n",
        "# to store the adjacency matrices, feature matrices, and QED values for each molecule,\n",
        "# respectively.\n",
        "adjacency_tensor, feature_tensor, qed_tensor = [], [], []\n",
        "\n",
        "# for idx in range(row_count):: This loop iterates through each row of the train_df DataFrame.\n",
        "for idx in range(row_count): # Use this for training sets less than 8000\n",
        "# for idx in range(8000):   # Use this for the original code\n",
        "\n",
        "    # adjacency, features = smiles_to_graph(train_df.loc[idx][\"smiles\"]): Inside the loop, this\n",
        "    # line calls the smiles_to_graph function (defined earlier in the code). This function takes\n",
        "    # a SMILES string (representing a molecule) and converts it into its graph representation\n",
        "     # (adjacency and feature matrices).\n",
        "    adjacency, features = smiles_to_graph(train_df.loc[idx][\"smiles\"])\n",
        "\n",
        "    # qed = train_df.loc[idx][\"qed\"]: This line extracts the QED value for the current molecule\n",
        "    # from the train_df DataFrame.\n",
        "    qed = train_df.loc[idx][\"qed\"]\n",
        "\n",
        "    # adjacency_tensor.append(adjacency), feature_tensor.append(features), qed_tensor.append(qed):\n",
        "    # These lines add the adjacency matrix, feature matrix, and QED value of the current molecule\n",
        "    # to their respective lists.\n",
        "    adjacency_tensor.append(adjacency)\n",
        "    feature_tensor.append(features)\n",
        "    qed_tensor.append(qed)\n",
        "\n",
        "# adjacency_tensor = np.array(adjacency_tensor), feature_tensor = np.array(feature_tensor),\n",
        "# qed_tensor = np.array(qed_tensor): These lines convert the lists into NumPy arrays, a format\n",
        "# suitable for using with Keras/TensorFlow.\n",
        "adjacency_tensor = np.array(adjacency_tensor)\n",
        "feature_tensor = np.array(feature_tensor)\n",
        "qed_tensor = np.array(qed_tensor)\n",
        "\n",
        "# This class defines a custom Keras layer. This is essential for processing the graph\n",
        "# representations of the molecules.\n",
        "class RelationalGraphConvLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        units=128,\n",
        "        activation=\"relu\",\n",
        "        use_bias=False,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
        "\n",
        "    # build: This method is called when the layer is first used and creates the layer's weights\n",
        "    # (kernel and optional bias).\n",
        "    # input_shape: This argument provides information about the shape of the data that will be\n",
        "    # fed into the layer. It's a tuple representing the dimensions of the input tensors\n",
        "     # (think of them as multi-dimensional arrays).\n",
        "    def build(self, input_shape):\n",
        "        # bond_dim: This variable is assigned the value of the bond dimension, which is\n",
        "        # extracted from the input_shape. It represents the number of different bond types in\n",
        "        # the molecules being considered (e.g., single, double, aromatic bonds).\n",
        "        bond_dim = input_shape[0][1]\n",
        "        #  atom_dim: Similar to bond_dim, this gets the atom dimension from the input_shape,\n",
        "        # representing the different atom types (e.g., carbon, oxygen, nitrogen).\n",
        "        atom_dim = input_shape[1][2]\n",
        "\n",
        "        # self.kernel: This is where the layer's main weights (also called the kernel) are stored.\n",
        "        # These weights are the learnable parameters of the layer, which will be adjusted during\n",
        "        # the training process to improve the model's performance.\n",
        "        # self.add_weight: This is a Keras function that creates a new weight variable and adds\n",
        "        # it to the layer's trainable parameters.\n",
        "\n",
        "        self.kernel = self.add_weight(\n",
        "\n",
        "            # shape: This argument defines the dimensions of the kernel. In this case, it's a 3D\n",
        "            # tensor with dimensions (bond_dim, atom_dim, self.units). The self.units value was\n",
        "            # specified when the layer was created and determines the number of output features\n",
        "            # for this layer.\n",
        "            shape=(bond_dim, atom_dim, self.units),\n",
        "\n",
        "            # initializer: This determines how the initial values of the weights are set. It's\n",
        "            # usually a function from the keras.initializers module. A good initialization\n",
        "            # strategy helps the model learn effectively.\n",
        "            initializer=self.kernel_initializer,\n",
        "\n",
        "            # regularizer: This optional argument adds a penalty to the loss function to prevent\n",
        "            # overfitting. This encourages the model to learn more general patterns and avoid\n",
        "            # memorizing the training data too closely.\n",
        "            regularizer=self.kernel_regularizer,\n",
        "\n",
        "            # trainable=True: This means that the kernel weights will be updated during training.\n",
        "            trainable=True,\n",
        "\n",
        "            # name=\"W\": This gives the weight variable a name, which can be helpful for debugging\n",
        "            # and understanding the model's structure.\n",
        "            name=\"W\",\n",
        "\n",
        "            # dtype=\"float32\": Specifies the data type of the weights, commonly 32-bit\n",
        "            # floating-point numbers.\n",
        "            dtype=\"float32\",\n",
        "        )\n",
        "\n",
        "        # This block is similar to the kernel creation but is only executed if self.use_bias is\n",
        "        # True. A bias is an additional learnable parameter that can be added to the layer's\n",
        "        # output, providing more flexibility during training.\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                shape=(bond_dim, 1, self.units),\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                trainable=True,\n",
        "                name=\"b\",\n",
        "                dtype=\"float32\",\n",
        "            )\n",
        "        # This set the built flag indicating that the layer has been built and its weights have\n",
        "        # been initialized. This prevents the build method from being called multiple times.\n",
        "        self.built = True\n",
        "\n",
        "    # call: This method defines the layer's forward pass, taking the adjacency and feature\n",
        "    # matrices as input and performing the graph convolution operation. It involves matrix\n",
        "    # multiplications, applying the activation function, and potentially adding a bias.\n",
        "    def call(self, inputs, training=False):\n",
        "        adjacency, features = inputs\n",
        "        # Aggregate information from neighbors\n",
        "        x = ops.matmul(adjacency, features[:, None])\n",
        "        # Apply linear transformation\n",
        "        x = ops.matmul(x, self.kernel)\n",
        "        if self.use_bias:\n",
        "            x += self.bias\n",
        "        # Reduce bond types dim\n",
        "        x_reduced = ops.sum(x, axis=1)\n",
        "        # Apply non-linear transformation\n",
        "        return self.activation(x_reduced)"
      ],
      "metadata": {
        "id": "WC7pZma179i8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Encoder and Decoder\n",
        "\n",
        "The Encoder takes as input a molecule's graph adjacency matrix and feature matrix.\n",
        "These features are processed via a Graph Convolution layer, then are flattened and\n",
        "processed by several Dense layers to derive `z_mean` and `log_var`, the\n",
        "latent-space representation of the molecule.\n",
        "\n",
        "**Graph Convolution layer**: The relational graph convolution layer implements\n",
        "non-linearly transformed neighbourhood aggregations. We can define these layers as\n",
        "follows:\n",
        "\n",
        "`H_hat**(l+1) = σ(D_hat**(-1) * A_hat * H_hat**(l+1) * W**(l))`\n",
        "\n",
        "Where `σ` denotes the non-linear transformation (commonly a ReLU activation), `A` the\n",
        "adjacency tensor, `H_hat**(l)` the feature tensor at the `l-th` layer, `D_hat**(-1)` the\n",
        "inverse diagonal degree tensor of `A_hat`, and `W_hat**(l)` the trainable weight tensor\n",
        "at the `l-th` layer. Specifically, for each bond type (relation), the degree tensor\n",
        "expresses, in the diagonal, the number of bonds attached to each atom.\n",
        "\n",
        "Source:\n",
        "[WGAN-GP with R-GCN for the generation of small molecular graphs](https://keras.io/examples/generative/wgan-graphs/))\n",
        "\n",
        "The Decoder takes as input the latent-space representation and predicts\n",
        "the graph adjacency matrix and feature matrix of the corresponding molecules."
      ],
      "metadata": {
        "id": "v0PfQL3m-86W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function builds the encoder part of the VAE. The encoder's job is to take a molecule\n",
        "# represented as a graph (defined by adjacency and features) and compress it into a\n",
        "# lower-dimensional representation called the latent space.\n",
        "def get_encoder(\n",
        "    gconv_units, latent_dim, adjacency_shape, feature_shape, dense_units, dropout_rate\n",
        "):\n",
        "    adjacency = layers.Input(shape=adjacency_shape)  # The adjacency matrix describes atom connections\n",
        "    features = layers.Input(shape=feature_shape)  # The feature matrix describes atom properties\n",
        "\n",
        "    # Propagate through one or more graph convolutional layers. These layers are designed to\n",
        "    # learn patterns in the graph structure of the molecule. Information from neighboring\n",
        "    # atoms is then aggregated to create a more informative representation of each atom.\n",
        "    features_transformed = features\n",
        "    for units in gconv_units:\n",
        "        features_transformed = RelationalGraphConvLayer(units)(\n",
        "            [adjacency, features_transformed]\n",
        "        )\n",
        "    # Reduce 2-D representation of molecule to 1-D. This essentially averages the feature\n",
        "    # values across all atoms in the molecue.\n",
        "    x = layers.GlobalAveragePooling1D()(features_transformed)\n",
        "\n",
        "    # Propagate through one or more densely connected layers. The output from pooling is passed\n",
        "    # through one or more dense (fully connected) layers with ReLU activation. These layers\n",
        "    # further process the information and learn complex relationships. layers.Dropout() is\n",
        "    # applied to prevent overfitting. This randomly sets a fraction of input units to 0 at each\n",
        "    #  update during training time, which helps prevent the model from relying too much on any\n",
        "    # single feature.\n",
        "    for units in dense_units:\n",
        "        x = layers.Dense(units, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Two dense layers are used to output z_mean and log_var. These represent the mean and\n",
        "    # logarithm of the variance of the molecule's distribution in the latent space. The latent\n",
        "    # space is a lower-dimensional representation of the molecule, capturing its essential\n",
        "    # features.\n",
        "    z_mean = layers.Dense(latent_dim, dtype=\"float32\", name=\"z_mean\")(x)\n",
        "    log_var = layers.Dense(latent_dim, dtype=\"float32\", name=\"log_var\")(x)\n",
        "\n",
        "    # The encoder model takes the adjacency and features as input and outputs z_mean and log_var.\n",
        "    encoder = keras.Model([adjacency, features], [z_mean, log_var], name=\"encoder\")\n",
        "\n",
        "    return encoder\n",
        "\n",
        "# This function builds the decoder part of the VAE. The decoder's task is to take a point from\n",
        "# the latent space (the compressed representation of a molecule) and reconstruct the original\n",
        "# molecule's graph representation.\n",
        "def get_decoder(dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape):\n",
        "\n",
        "    # Takes a point in the latent space (latent_inputs) as input.\n",
        "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "\n",
        "    # The latent space representation is passed through one or more dense layers with tanh\n",
        "    # activation and dropout for regularization.\n",
        "    x = latent_inputs\n",
        "    for units in dense_units:\n",
        "        x = layers.Dense(units, activation=\"tanh\")(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Reconstruction: Two separate branches are created to reconstruct:\n",
        "      # Adjacency Matrix: Dense layer followed by reshape and softmax to predict the\n",
        "      # adjacency matrix. The adjacency matrix is symmetrized to ensure the connections\n",
        "      # between atoms are consistent.\n",
        "      # Feature Matrix: Dense layer followed by reshape and softmax to predict the feature matrix.\n",
        "\n",
        "    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)\n",
        "    x_adjacency = layers.Dense(np.prod(adjacency_shape))(x)\n",
        "    x_adjacency = layers.Reshape(adjacency_shape)(x_adjacency)\n",
        "    # Symmetrify tensors in the last two dimensions\n",
        "    x_adjacency = (x_adjacency + ops.transpose(x_adjacency, (0, 1, 3, 2))) / 2\n",
        "    x_adjacency = layers.Softmax(axis=1)(x_adjacency)\n",
        "\n",
        "    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)\n",
        "    x_features = layers.Dense(np.prod(feature_shape))(x)\n",
        "    x_features = layers.Reshape(feature_shape)(x_features)\n",
        "    x_features = layers.Softmax(axis=2)(x_features)\n",
        "\n",
        "    # The decoder model takes latent_inputs as input and outputs the reconstructed\n",
        "    # x_adjacency and x_features.\n",
        "    decoder = keras.Model(\n",
        "        latent_inputs, outputs=[x_adjacency, x_features], name=\"decoder\"\n",
        "    )\n",
        "\n",
        "    return decoder"
      ],
      "metadata": {
        "id": "eEAQRyo38gTU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Sampling Layer\n",
        "This class defines a custom Keras layer called Sampling. This class's job is to sample points from the latent space. In simpler terms, the Sampling layer takes the encoder's output and adds some random noise to it, creating a point in the latent space. This point is then fed to the decoder to generate a new molecule. The randomness introduced by this layer is what allows the VAE to create diverse and novel molecules.\n",
        "\n",
        "### What is the Latent Space?\n",
        "In a VAE, the latent space is a lower-dimensional representation of the input data (in this case, molecules). Think of it as a compressed version of the molecules, where similar molecules are clustered together. The Sampling layer helps the VAE generate new molecules by randomly picking points within this latent space."
      ],
      "metadata": {
        "id": "6tguCkSw_Dfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    # __init__(self, seed=None, **kwargs):\n",
        "    # This is the constructor of the class. It initializes the layer.\n",
        "\n",
        "    # seed:\n",
        "    # An optional argument to set a seed for the random number generator, making the sampling\n",
        "    # process reproducible.\n",
        "\n",
        "    # **kwargs:\n",
        "    # Allows you to pass additional keyword arguments to the parent class (layers.Layer).\n",
        "    def __init__(self, seed=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.seed_generator = keras.random.SeedGenerator(seed)\n",
        "\n",
        "    # call(self, inputs):\n",
        "    # This is the core of the layer's functionality. It's executed when the layer is called\n",
        "    # during the model's forward pass.\n",
        "    def call(self, inputs):\n",
        "        # inputs: A tuple containing z_mean and z_log_var. These are the outputs of the encoder,\n",
        "        # representing the mean and logarithm of the variance of the latent distribution.\n",
        "        z_mean, z_log_var = inputs\n",
        "        # batch, dim = ops.shape(z_log_var): Gets the batch size (batch) and dimensionality\n",
        "        # (dim) of the latent space from z_log_var.\n",
        "        batch, dim = ops.shape(z_log_var)\n",
        "        # epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator): This line\n",
        "        # is where the actual sampling happens. It draws random samples (noise) from a normal\n",
        "        # distribution with the shape of the latent space. The seed ensures reproducibility if\n",
        "        # provided.\n",
        "        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
        "        # return z_mean + ops.exp(0.5 * z_log_var) * epsilon: This is the \"reparameterization\n",
        "        # trick.\" It combines the mean (z_mean), the standard deviation\n",
        "        # (calculated from z_log_var), and the random noise (epsilon) to get a sample from the\n",
        "        #  latent space. This trick is important for making the VAE trainable using backpropagation.\n",
        "        return z_mean + ops.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "do-mF7iJ8qXG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the VAE\n",
        "\n",
        "This model is trained to optimize four losses:\n",
        "\n",
        "* Categorical crossentropy\n",
        "* KL divergence loss\n",
        "* Property prediction loss\n",
        "* Graph loss (gradient penalty)\n",
        "\n",
        "The categorical crossentropy loss function measures the model's reconstruction accuracy. The Property prediction loss estimates the mean squared error between predicted and actual properties after running the latent representation through a property prediction model. The property prediction of the model is optimized via binary crossentropy. The gradient penalty is further guided by the model's property (QED) prediction.\n",
        "\n",
        "A gradient penalty is an alternative soft constraint on the 1-Lipschitz continuity as an improvement upon the gradient clipping scheme from the original neural network (\"1-Lipschitz continuity\" means that the norm of the gradient is at most 1 at every single point of the function). It adds a regularization term to the loss function."
      ],
      "metadata": {
        "id": "XUia_B3Q_I6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoleculeGenerator(keras.Model):\n",
        "    def __init__(self, encoder, decoder, max_len, seed=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder # Takes a molecule and compresses it into a code (latent space representation)\n",
        "        self.decoder = decoder # Takes the code reconstructs the molecule\n",
        "        self.property_prediction_layer = layers.Dense(1) # Predicts molecular properties from the code\n",
        "        self.max_len = max_len # Maximum length of the molecules's represenation\n",
        "        self.seed_generator = keras.random.SeedGenerator(seed) # For reproducible results\n",
        "        self.sampling_layer = Sampling(seed=seed) # Adds randomness for generating diverse molecules\n",
        "\n",
        "        # train_total_loss_tracker and val_total_loss_tracker track the model's performance during training.\n",
        "        self.train_total_loss_tracker = keras.metrics.Mean(name=\"train_total_loss\")\n",
        "        self.val_total_loss_tracker = keras.metrics.Mean(name=\"val_total_loss\")\n",
        "\n",
        "    # This function defines how the model learns from data. It's like training the factory\n",
        "    # workers to improve their skills. It takes a batch of data (data), which includes molecule\n",
        "    # representations and properties. It calculates the loss, which measures how well the model\n",
        "    # is doing. It updates the model's parameters to minimize the loss, making it better at\n",
        "    # generating molecules.\n",
        "    def train_step(self, data):\n",
        "        # Unpack the input data to get the adjacency_tensor, feature_tensor, and qed_tensor\n",
        "        # adjacency_tensor and feature_tensor represent the actual molecule's graph structure\n",
        "        # and are combined into graph_real. qed_tensor holds the target property values\n",
        "        # (Quantitative Estimate of Drug-likeness) for the molecules in the batch.\n",
        "        adjacency_tensor, feature_tensor, qed_tensor = data[0]\n",
        "        graph_real = [adjacency_tensor, feature_tensor]\n",
        "        self.batch_size = ops.shape(qed_tensor)[0]  # This is set based on the size of qed_tensor\n",
        "\n",
        "        # tf.GradientTape() is used to record operations for automatic differentiation,\n",
        "        # which is necessary for calculating gradients during backpropagation.\n",
        "        with tf.GradientTape() as tape:\n",
        "            # The model (self) is called with graph_real to get the:\n",
        "                # z_mean and z_log_var: Latent space representation of the input molecule.\n",
        "                # qed_pred: Predicted QED value.\n",
        "                # gen_adjacency and gen_features: Reconstructed graph representation.\n",
        "                # graph_generated combines the reconstructed adjacency and feature tensors.\n",
        "                # self._compute_loss is called to calculate the total loss\n",
        "            z_mean, z_log_var, qed_pred, gen_adjacency, gen_features = self(\n",
        "                graph_real, training=True\n",
        "            )\n",
        "            graph_generated = [gen_adjacency, gen_features]\n",
        "            total_loss = self._compute_loss(\n",
        "                z_log_var, z_mean, qed_tensor, qed_pred, graph_real, graph_generated\n",
        "            )\n",
        "        # tape.gradient calculates the gradients of the total_loss with respect to the model's\n",
        "        # trainable weights.\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        # self.optimizer.apply_gradients updates the model's weights based on the calculated\n",
        "        # gradients, aiming to minimize the loss.\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        # The train_total_loss_tracker is updated with the current total_loss.\n",
        "        # The function returns a dictionary containing the current average training loss.\n",
        "        self.train_total_loss_tracker.update_state(total_loss)\n",
        "        return {\"loss\": self.train_total_loss_tracker.result()}\n",
        "\n",
        "    # _compute_loss is a helper function that calculates the total loss based on:\n",
        "        # kl_loss: Measures how well the latent space captures information.\n",
        "        # property_loss: Measures how accurately the model predicts molecular properties.\n",
        "        # graph_loss: Measures the quality of the generated molecule graphs.\n",
        "        # adjacency_loss, features_loss: Measure the accuracy of the reconstructed molecule parts.\n",
        "    def _compute_loss(\n",
        "        # self: Refers to the MoleculeGenerator instance itself.\n",
        "        # z_log_var: Logarithm of the variance of the latent space representation.\n",
        "        # z_mean: Mean of the latent space representation.\n",
        "        # qed_true: The true Quantitative Estimate of Drug-likeness (QED) values for the input\n",
        "        # molecules.\n",
        "        # qed_pred: The predicted QED values by the model.\n",
        "        # graph_real: The actual graph representation of the input molecules, containing\n",
        "        # adjacency_real (connectivity) and features_real (atom properties).\n",
        "        # graph_generated: The graph representation generated by the decoder, containing\n",
        "        # adjacency_gen and features_gen.\n",
        "        self, z_log_var, z_mean, qed_true, qed_pred, graph_real, graph_generated\n",
        "    ):\n",
        "        adjacency_real, features_real = graph_real\n",
        "        adjacency_gen, features_gen = graph_generated\n",
        "\n",
        "        # adjacency_loss: Measures how well the generated adjacency matrix matches the real\n",
        "        # adjacency matrix using categorical cross-entropy. This essentially quantifies the\n",
        "        # difference in connectivity between the generated and real molecules.\n",
        "        adjacency_loss = ops.mean(\n",
        "            ops.sum(\n",
        "                keras.losses.categorical_crossentropy(\n",
        "                    adjacency_real, adjacency_gen, axis=1\n",
        "                ),\n",
        "                axis=(1, 2),\n",
        "            )\n",
        "        )\n",
        "        # Similar to `adjacency_loss`, but it measures the difference between the generated and\n",
        "        # real feature matrices (atom properties) using categorical cross-entropy.\n",
        "        features_loss = ops.mean(\n",
        "            ops.sum(\n",
        "                keras.losses.categorical_crossentropy(features_real, features_gen),\n",
        "                axis=(1),\n",
        "            )\n",
        "        )\n",
        "        # kl_loss stands for Kullback-Leibler divergence loss. It encourages the latent space\n",
        "        # distribution to be close to a standard normal distribution, which helps in generating\n",
        "        # diverse and meaningful molecules.\n",
        "        kl_loss = -0.5 * ops.sum(\n",
        "            1 + z_log_var - z_mean**2 - ops.minimum(ops.exp(z_log_var), 1e6), 1\n",
        "        )\n",
        "        kl_loss = ops.mean(kl_loss)\n",
        "\n",
        "        # property_loss measures how well the model predicts the QED property of the molecules\n",
        "        # using binary cross-entropy. This ensures the generated molecules have desired properties.\n",
        "        property_loss = ops.mean(\n",
        "            keras.losses.binary_crossentropy(qed_true, ops.squeeze(qed_pred, axis=1))\n",
        "        )\n",
        "\n",
        "        # graph_loss is calculated using a separate function, `_gradient_penalty`. It acts as a\n",
        "        # regularization term to ensure the generated molecules are realistic and avoid\n",
        "        # unnatural structures.\n",
        "        graph_loss = self._gradient_penalty(graph_real, graph_generated)\n",
        "\n",
        "        # This returns the total loss, which is the sum of all the individual loss components.\n",
        "        # This total loss is what the model tries to minimize during training.\n",
        "        return kl_loss + property_loss + graph_loss + adjacency_loss + features_loss\n",
        "\n",
        "    # The _gradient_penalty function adds a penalty to the loss to ensure the latent space is\n",
        "    # smooth and continuous. This helps in generating more realistic molecules.\n",
        "    def _gradient_penalty(self, graph_real, graph_generated):\n",
        "\n",
        "        # Unpack graphs\n",
        "        adjacency_real, features_real = graph_real # Represents connectivity of atoms in the molecules\n",
        "        adjacency_generated, features_generated = graph_generated # Represents the properties of the atoms in the molecules\n",
        "\n",
        "        # Generate interpolated graphs (adjacency_interp and features_interp)\n",
        "        # alpha: A random number between 0 and 1 is generated for each molecule in the batch.\n",
        "        # This alpha is used to create interpolated graphs.\n",
        "        alpha = keras.random.uniform(shape=(self.batch_size,), seed=self.seed_generator)\n",
        "        alpha = ops.reshape(alpha, (self.batch_size, 1, 1, 1))\n",
        "\n",
        "        # adjacency_interp, features_interp: These are created by linearly interpolating between\n",
        "        # the real and generated graphs using alpha. This essentially creates a mix of the real\n",
        "        # and generated molecules.\n",
        "        adjacency_interp = (adjacency_real * alpha) + (\n",
        "            1.0 - alpha\n",
        "        ) * adjacency_generated\n",
        "        alpha = ops.reshape(alpha, (self.batch_size, 1, 1))\n",
        "        features_interp = (features_real * alpha) + (1.0 - alpha) * features_generated\n",
        "\n",
        "        # Compute the logits of interpolated graphs\n",
        "        # tf.GradientTape() is used to record operations so that gradients can be calculated later.\n",
        "        # tape.watch() tells the tape to watch the interpolated graphs for changes.\n",
        "        # self(...): The model is called with the interpolated graphs to get the logits\n",
        "         # (which are intermediate outputs of the model).\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(adjacency_interp)\n",
        "            tape.watch(features_interp)\n",
        "            _, _, logits, _, _ = self(\n",
        "                [adjacency_interp, features_interp], training=True\n",
        "            )\n",
        "\n",
        "        # Compute the gradients with respect to the interpolated graphs\n",
        "        # tape.gradient() calculates the gradients of the logits with respect to the interpolated graphs.\n",
        "        grads = tape.gradient(logits, [adjacency_interp, features_interp])\n",
        "\n",
        "        # Compute the gradient penalty\n",
        "        # grads_adjacency_penalty, grads_features_penalty: These calculate the penalty based on\n",
        "        # how far the gradients deviate from 1 (which is related to the 1-Lipschitz constraint).\n",
        "        grads_adjacency_penalty = (1 - ops.norm(grads[0], axis=1)) ** 2\n",
        "        grads_features_penalty = (1 - ops.norm(grads[1], axis=2)) ** 2\n",
        "\n",
        "        # The final return value is the mean of these penalties.\n",
        "        return ops.mean(\n",
        "            ops.mean(grads_adjacency_penalty, axis=(-2, -1))\n",
        "            + ops.mean(grads_features_penalty, axis=(-1))\n",
        "        )\n",
        "    # This is the part where the 'factory' produces new molecules. Random points in the latent\n",
        "    # space are sampled and the decoder is then used  to generate corresponding molecule graphs.\n",
        "    # The graphs are then converted into RDKit molecule objects, which represent the generated\n",
        "    # molecules.  It takes a batch_size as input, which determines how many new molecules to\n",
        "    # generate at once.\n",
        "    def inference(self, batch_size):\n",
        "        # keras.random.normal: This line generates random points (z) in the latent space. These\n",
        "        # points will be the starting point for creating new molecules.\n",
        "        z = keras.random.normal(\n",
        "\n",
        "            # shape=(batch_size, LATENT_DIM): This specifies the shape of the random points.\n",
        "            # It creates a matrix with batch_size rows (one for each molecule) and LATENT_DIM\n",
        "            # columns (representing the dimensions of the latent space).\n",
        "            # seed=self.seed_generator: This ensures that the random number generation is\n",
        "            # reproducible, meaning you'll get the same results if you run the code again with\n",
        "            # the same seed.\n",
        "            shape=(batch_size, LATENT_DIM), seed=self.seed_generator\n",
        "        )\n",
        "        # model.decoder.predict(z): This line feeds the random points (z) from the latent space\n",
        "        # into the decoder part of the VAE. The decoder's job is to translate these points\n",
        "        # back into a representation of a molecule.\n",
        "        # reconstruction_adjacency, reconstruction_features: The decoder outputs two things: the\n",
        "        # reconstruction_adjacency (which describes how atoms are connected in the molecule) and\n",
        "        # reconstruction_features (which describes the properties of each atom).\n",
        "        reconstruction_adjacency, reconstruction_features = model.decoder.predict(z)\n",
        "\n",
        "        # ops.argmax(...) and ops.one_hot(...): These lines convert the reconstruction_adjacency\n",
        "        # into a one-hot encoded adjacency tensor. This format is easier for the model to work with.\n",
        "        adjacency = ops.argmax(reconstruction_adjacency, axis=1)\n",
        "        adjacency = ops.one_hot(adjacency, num_classes=BOND_DIM, axis=1)\n",
        "\n",
        "        # adjacency = adjacency * ...: This line removes any potential self-loops in the\n",
        "        # adjacency matrix. Self-loops represent an atom being connected to itself, which is\n",
        "        # chemically invalid.\n",
        "        adjacency = adjacency * (1.0 - ops.eye(NUM_ATOMS, dtype=\"float32\")[None, None])\n",
        "\n",
        "        # Similar to the adjacency matrix processing, these lines convert the reconstruction_features\n",
        "        # into a one-hot encoded tensor, making it easier to handle.\n",
        "        features = ops.argmax(reconstruction_features, axis=2)\n",
        "        features = ops.one_hot(features, num_classes=ATOM_DIM, axis=2)\n",
        "\n",
        "        # graph_to_molecule(...): This line takes the processed adjacency and feature matrices\n",
        "        # and converts them back into RDKit molecule objects. RDKit is a library for working with\n",
        "        # chemical data. The code iterates through the batch and creates a list of RDKit molecule\n",
        "        # objects, one for each generated molecule.\n",
        "        return [\n",
        "            graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n",
        "            for i in range(batch_size)\n",
        "        ]\n",
        "\n",
        "    # call() defines how the model processes input and produces output. It's automatically\n",
        "    # executed when you use the model to make predictions or during training.\n",
        "    def call(self, inputs):\n",
        "        # z_mean, log_var = self.encoder(inputs): This line takes the inputs (which represent a\n",
        "        # molecule's graph) and feeds them into the encoder. The encoder processes the graph and\n",
        "        # outputs two important things: z_mean and log_var. These represent the molecule's\n",
        "        # compressed representation in the latent space (a lower-dimensional space where similar\n",
        "        # molecules are clustered together).\n",
        "        z_mean, log_var = self.encoder(inputs)\n",
        "\n",
        "        # adjacency_tensor, feature_tensor = inputs  # Unpack inputs\n",
        "        # z_mean, log_var = self.encoder([adjacency_tensor, feature_tensor])  # Pass to encoder\n",
        "\n",
        "        # z = self.sampling_layer([z_mean, log_var]): This line uses the sampling_layer to introduce\n",
        "        # some randomness. It takes z_mean and log_var and adds random noise to them, creating a\n",
        "        # point (z) in the latent space. This randomness is crucial for generating diverse and\n",
        "        # novel molecules.\n",
        "        z = self.sampling_layer([z_mean, log_var])\n",
        "\n",
        "        # gen_adjacency, gen_features = self.decoder(z): The sampled point z is then passed to\n",
        "        # the decoder. The decoder's job is to reconstruct the molecule's graph from this latent\n",
        "        # space representation. It outputs gen_adjacency and gen_features, which represent the\n",
        "        # connections between atoms and the properties of each atom in the generated molecule.\n",
        "        gen_adjacency, gen_features = self.decoder(z)\n",
        "\n",
        "        # property_pred = self.property_prediction_layer(z_mean): This line uses the\n",
        "        # property_prediction_layer to estimate the properties of the generated molecule based\n",
        "        # on its latent space representation (z_mean).\n",
        "        property_pred = self.property_prediction_layer(z_mean)\n",
        "\n",
        "        # return z_mean, log_var, property_pred, gen_adjacency, gen_features: The function returns\n",
        "        # all the important information: the latent space representation (z_mean, log_var), the\n",
        "        # predicted property (property_pred), and the generated molecule's graph\n",
        "        # (gen_adjacency, gen_features).\n",
        "        return z_mean, log_var, property_pred, gen_adjacency, gen_features"
      ],
      "metadata": {
        "id": "57zPFYe_8xi_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "This code block focuses on training the Variational Autoencoder (VAE) model to generate new molecules. It involves setting up the optimizer, creating the encoder and decoder components, building the overall model, and finally training it."
      ],
      "metadata": {
        "id": "ciyAVG2Y_OK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vae_optimizer: This variable stores the optimization algorithm used to train the VAE model.\n",
        "    # keras.optimizers.Adam: This is a popular optimization algorithm in deep learning.\n",
        "    # learning_rate=VAE_LR: This sets the learning rate, a hyperparameter that controls how much\n",
        "    # the model's parameters are adjusted during each training step. VAE_LR is a variable defined\n",
        "    # in the 'Defining Hyperparameters' code block section of this code.\n",
        "vae_optimizer = keras.optimizers.Adam(learning_rate=VAE_LR)\n",
        "\n",
        "# encoder: This variable stores the encoder part of the VAE.\n",
        "# get_encoder: This function (defined earlier) builds the encoder network.\n",
        "# The parameters (gconv_units, adjacency_shape, feature_shape, latent_dim, dense_units,\n",
        "# dropout_rate) define the architecture of the encoder network, such as the number of layers,\n",
        "# their sizes, and other properties.\n",
        "encoder = get_encoder(\n",
        "    gconv_units=[9],\n",
        "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
        "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
        "    latent_dim=LATENT_DIM,\n",
        "    dense_units=[512],\n",
        "    dropout_rate=0.0,\n",
        ")\n",
        "\n",
        "# decoder: This variable stores the decoder part of the VAE.\n",
        "# get_decoder: Similar to get_encoder, this function (defined earlier) builds the decoder network.\n",
        "# The parameters (dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape) define\n",
        "# the architecture of the decoder.\n",
        "decoder = get_decoder(\n",
        "    dense_units=[128, 256, 512],\n",
        "    dropout_rate=0.2,\n",
        "    latent_dim=LATENT_DIM,\n",
        "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
        "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
        ")\n",
        "\n",
        "# model: This variable stores the complete VAE model.\n",
        "    # MoleculeGenerator: This is the class (defined earlier) that combines the encoder, decoder,\n",
        "    # and other components to form the VAE.\n",
        "    # encoder, decoder, MIN_MOLSIZE: These are passed as arguments to create the VAE model.\n",
        "model = MoleculeGenerator(encoder, decoder, MIN_MOLSIZE)\n",
        "\n",
        "# model.compile: This prepares the model for training by specifying the optimizer.\n",
        "    # vae_optimizer: The previously defined Adam optimizer.\n",
        "model.compile(vae_optimizer)\n",
        "\n",
        "# model.fit: This starts the training process.\n",
        "    # [adjacency_tensor, feature_tensor, qed_tensor]: This is the training data.\n",
        "    # adjacency_tensor: Represents the connectivity of atoms in the molecules.\n",
        "    # feature_tensor: Represents properties of the atoms in the molecules.\n",
        "    # qed_tensor: Represents the Quantitative Estimate of Drug-likeness, a property of the molecules\n",
        "    # used for training.\n",
        "    # epochs=500: This sets the number of training epochs. One epoch means the model has seen the\n",
        "    # entire training dataset once.\n",
        "history = model.fit([adjacency_tensor, feature_tensor, qed_tensor], epochs=600)"
      ],
      "metadata": {
        "id": "kVfL-9od_QHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a66e2c-8b18-4584-fc0b-1b1f439a7128"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 1526.4982\n",
            "Epoch 2/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 1491.7333\n",
            "Epoch 3/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 1453.6318\n",
            "Epoch 4/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 1377.8082\n",
            "Epoch 5/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 1305.8412\n",
            "Epoch 6/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1201.7336\n",
            "Epoch 7/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1067.0481\n",
            "Epoch 8/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 872.5327\n",
            "Epoch 9/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 727.9157\n",
            "Epoch 10/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 544.6755\n",
            "Epoch 11/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 440.3062\n",
            "Epoch 12/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 330.7890\n",
            "Epoch 13/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 255.2581\n",
            "Epoch 14/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 217.1199\n",
            "Epoch 15/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 201.7419\n",
            "Epoch 16/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 193.2486\n",
            "Epoch 17/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 190.6725\n",
            "Epoch 18/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 189.4276\n",
            "Epoch 19/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 188.8027\n",
            "Epoch 20/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 188.0341\n",
            "Epoch 21/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 186.8295\n",
            "Epoch 22/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 185.2243\n",
            "Epoch 23/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 183.2984\n",
            "Epoch 24/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 181.0750\n",
            "Epoch 25/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 178.8760\n",
            "Epoch 26/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 176.7389\n",
            "Epoch 27/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 174.1322\n",
            "Epoch 28/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 171.2326\n",
            "Epoch 29/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 169.0472\n",
            "Epoch 30/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 166.7237\n",
            "Epoch 31/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 163.8787\n",
            "Epoch 32/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 161.8585\n",
            "Epoch 33/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 159.7159\n",
            "Epoch 34/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 157.7905\n",
            "Epoch 35/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - loss: 156.2435\n",
            "Epoch 36/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 154.5600\n",
            "Epoch 37/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 152.8665\n",
            "Epoch 38/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 151.0506\n",
            "Epoch 39/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 148.8057\n",
            "Epoch 40/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - loss: 146.6044\n",
            "Epoch 41/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - loss: 143.9040\n",
            "Epoch 42/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 142.1205\n",
            "Epoch 43/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - loss: 140.3948\n",
            "Epoch 44/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - loss: 138.9437\n",
            "Epoch 45/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - loss: 137.6727\n",
            "Epoch 46/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 136.3707\n",
            "Epoch 47/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 135.0685\n",
            "Epoch 48/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - loss: 133.9584\n",
            "Epoch 49/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 133.0135\n",
            "Epoch 50/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - loss: 131.8550\n",
            "Epoch 51/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 130.8340\n",
            "Epoch 52/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 130.0628\n",
            "Epoch 53/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 128.7108\n",
            "Epoch 54/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 127.8609\n",
            "Epoch 55/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 127.0492\n",
            "Epoch 56/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 126.5853\n",
            "Epoch 57/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 125.9137\n",
            "Epoch 58/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 124.9506\n",
            "Epoch 59/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - loss: 124.8824\n",
            "Epoch 60/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 123.9469\n",
            "Epoch 61/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 123.0594\n",
            "Epoch 62/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 122.9690\n",
            "Epoch 63/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 123.1416\n",
            "Epoch 64/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 123.2714\n",
            "Epoch 65/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 122.8431\n",
            "Epoch 66/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 122.0429\n",
            "Epoch 67/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 121.3442\n",
            "Epoch 68/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 121.5246\n",
            "Epoch 69/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 121.4694\n",
            "Epoch 70/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 121.0021\n",
            "Epoch 71/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 120.1483\n",
            "Epoch 72/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 121.6256\n",
            "Epoch 73/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 119.7309\n",
            "Epoch 74/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 121.8647\n",
            "Epoch 75/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 123.0137\n",
            "Epoch 76/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 122.2590\n",
            "Epoch 77/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 124.9755\n",
            "Epoch 78/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 129.8279\n",
            "Epoch 79/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 119.8438\n",
            "Epoch 80/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 120.8133\n",
            "Epoch 81/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 122.4239\n",
            "Epoch 82/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 122.5209\n",
            "Epoch 83/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 120.5100\n",
            "Epoch 84/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 121.0468\n",
            "Epoch 85/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 121.4760\n",
            "Epoch 86/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 121.8613\n",
            "Epoch 87/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 121.5960\n",
            "Epoch 88/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 121.7061\n",
            "Epoch 89/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 121.6505\n",
            "Epoch 90/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 121.9650\n",
            "Epoch 91/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 122.9258\n",
            "Epoch 92/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 121.4158\n",
            "Epoch 93/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 121.7480\n",
            "Epoch 94/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 121.6620\n",
            "Epoch 95/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 121.6662\n",
            "Epoch 96/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 121.7829\n",
            "Epoch 97/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 121.1993\n",
            "Epoch 98/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 121.1784\n",
            "Epoch 99/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 121.0500\n",
            "Epoch 100/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 120.5994\n",
            "Epoch 101/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 119.8979\n",
            "Epoch 102/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 120.5472\n",
            "Epoch 103/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 119.5548\n",
            "Epoch 104/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 120.3480\n",
            "Epoch 105/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 119.9705\n",
            "Epoch 106/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 119.7110\n",
            "Epoch 107/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 119.4994\n",
            "Epoch 108/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 118.8516\n",
            "Epoch 109/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 119.0744\n",
            "Epoch 110/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 127.5192\n",
            "Epoch 111/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 119.4405\n",
            "Epoch 112/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 119.6781\n",
            "Epoch 113/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 118.4551\n",
            "Epoch 114/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 119.1979\n",
            "Epoch 115/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 119.6268\n",
            "Epoch 116/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 119.3515\n",
            "Epoch 117/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 118.6034\n",
            "Epoch 118/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 118.3156\n",
            "Epoch 119/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 128.1576\n",
            "Epoch 120/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 119.9339\n",
            "Epoch 121/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 119.0989\n",
            "Epoch 122/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 119.2620\n",
            "Epoch 123/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 119.4372\n",
            "Epoch 124/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 119.9156\n",
            "Epoch 125/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 119.4941\n",
            "Epoch 126/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 119.5293\n",
            "Epoch 127/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 119.2920\n",
            "Epoch 128/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - loss: 119.6311\n",
            "Epoch 129/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 119.4820\n",
            "Epoch 130/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - loss: 119.7231\n",
            "Epoch 131/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 119.0434\n",
            "Epoch 132/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - loss: 119.6314\n",
            "Epoch 133/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - loss: 119.2924\n",
            "Epoch 134/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - loss: 118.9546\n",
            "Epoch 135/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - loss: 119.1831\n",
            "Epoch 136/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - loss: 119.7113\n",
            "Epoch 137/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - loss: 118.6565\n",
            "Epoch 138/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 118.4864\n",
            "Epoch 139/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - loss: 118.4522\n",
            "Epoch 140/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 118.5091\n",
            "Epoch 141/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 121.4421\n",
            "Epoch 142/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 119.6535\n",
            "Epoch 143/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 117.9842\n",
            "Epoch 144/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 117.7924\n",
            "Epoch 145/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 118.4441\n",
            "Epoch 146/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 118.0603\n",
            "Epoch 147/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 117.2452\n",
            "Epoch 148/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 118.1665\n",
            "Epoch 149/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 116.2855\n",
            "Epoch 150/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 118.2596\n",
            "Epoch 151/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 119.7958\n",
            "Epoch 152/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 116.8856\n",
            "Epoch 153/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 116.4473\n",
            "Epoch 154/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 117.7418\n",
            "Epoch 155/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 117.8923\n",
            "Epoch 156/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 134.9843\n",
            "Epoch 157/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 117.1427\n",
            "Epoch 158/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 118.4099\n",
            "Epoch 159/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 118.6533\n",
            "Epoch 160/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 119.3854\n",
            "Epoch 161/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 119.3086\n",
            "Epoch 162/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 119.7519\n",
            "Epoch 163/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 120.2409\n",
            "Epoch 164/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 119.4619\n",
            "Epoch 165/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 120.2970\n",
            "Epoch 166/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 120.5805\n",
            "Epoch 167/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 120.5408\n",
            "Epoch 168/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 120.8947\n",
            "Epoch 169/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 121.0982\n",
            "Epoch 170/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 119.3673\n",
            "Epoch 171/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 121.2463\n",
            "Epoch 172/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 120.4931\n",
            "Epoch 173/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 120.3858\n",
            "Epoch 174/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 120.4346\n",
            "Epoch 175/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 120.1757\n",
            "Epoch 176/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 119.3865\n",
            "Epoch 177/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 119.3241\n",
            "Epoch 178/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 119.2853\n",
            "Epoch 179/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 118.0513\n",
            "Epoch 180/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 119.1906\n",
            "Epoch 181/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 118.5577\n",
            "Epoch 182/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 118.9179\n",
            "Epoch 183/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 116.8003\n",
            "Epoch 184/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 117.6654\n",
            "Epoch 185/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 145.2250\n",
            "Epoch 186/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 118.1301\n",
            "Epoch 187/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 118.4190\n",
            "Epoch 188/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - loss: 119.7856\n",
            "Epoch 189/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 120.8765\n",
            "Epoch 190/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 119.6733\n",
            "Epoch 191/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 120.4470\n",
            "Epoch 192/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 121.9821\n",
            "Epoch 193/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 120.3573\n",
            "Epoch 194/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 121.5864\n",
            "Epoch 195/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - loss: 122.6809\n",
            "Epoch 196/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 121.5783\n",
            "Epoch 197/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 121.7878\n",
            "Epoch 198/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 122.0120\n",
            "Epoch 199/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 121.9729\n",
            "Epoch 200/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 120.8068\n",
            "Epoch 201/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 121.4315\n",
            "Epoch 202/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 121.5384\n",
            "Epoch 203/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 119.7427\n",
            "Epoch 204/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 120.1376\n",
            "Epoch 205/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 119.7875\n",
            "Epoch 206/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 118.9832\n",
            "Epoch 207/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 117.2095\n",
            "Epoch 208/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 118.6944\n",
            "Epoch 209/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 118.0614\n",
            "Epoch 210/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 116.9873\n",
            "Epoch 211/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 117.9251\n",
            "Epoch 212/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 115.4423\n",
            "Epoch 213/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 115.8560\n",
            "Epoch 214/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 114.4585\n",
            "Epoch 215/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 116.6806\n",
            "Epoch 216/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - loss: 116.3958\n",
            "Epoch 217/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 111.9583\n",
            "Epoch 218/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 113.8064\n",
            "Epoch 219/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - loss: 456.6516\n",
            "Epoch 220/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 110.4621\n",
            "Epoch 221/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - loss: 114.4030\n",
            "Epoch 222/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - loss: 115.4455\n",
            "Epoch 223/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 113.5656\n",
            "Epoch 224/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - loss: 115.6242\n",
            "Epoch 225/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - loss: 116.4331\n",
            "Epoch 226/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 116.5783\n",
            "Epoch 227/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - loss: 116.3755\n",
            "Epoch 228/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - loss: 115.9315\n",
            "Epoch 229/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 117.8563\n",
            "Epoch 230/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - loss: 116.3503\n",
            "Epoch 231/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - loss: 116.9847\n",
            "Epoch 232/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 122.1574\n",
            "Epoch 233/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - loss: 118.8749\n",
            "Epoch 234/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 116.3417\n",
            "Epoch 235/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 117.8706\n",
            "Epoch 236/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 121.9984\n",
            "Epoch 237/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 116.3825\n",
            "Epoch 238/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 114.1277\n",
            "Epoch 239/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 115.4768\n",
            "Epoch 240/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 113.9300\n",
            "Epoch 241/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 118.8451\n",
            "Epoch 242/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 113.6715\n",
            "Epoch 243/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 117.6037\n",
            "Epoch 244/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 111.2104\n",
            "Epoch 245/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 113.7549\n",
            "Epoch 246/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 112.9075\n",
            "Epoch 247/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 110.1301\n",
            "Epoch 248/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 108.0182\n",
            "Epoch 249/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 108.5144\n",
            "Epoch 250/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 109.7707\n",
            "Epoch 251/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 106.0971\n",
            "Epoch 252/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 109.6925\n",
            "Epoch 253/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 107.9685\n",
            "Epoch 254/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 106.7976\n",
            "Epoch 255/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 109.6087\n",
            "Epoch 256/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 104.3421\n",
            "Epoch 257/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 106.9709\n",
            "Epoch 258/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 106.4678\n",
            "Epoch 259/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 105.5749\n",
            "Epoch 260/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 103.4614\n",
            "Epoch 261/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 103.3940\n",
            "Epoch 262/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 104.3966\n",
            "Epoch 263/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 103.3473\n",
            "Epoch 264/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 102.1442\n",
            "Epoch 265/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 100.5108\n",
            "Epoch 266/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 101.3666\n",
            "Epoch 267/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 105.7325\n",
            "Epoch 268/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 103.9006\n",
            "Epoch 269/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 109.4408\n",
            "Epoch 270/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 104.5976\n",
            "Epoch 271/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 109.8325\n",
            "Epoch 272/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 101.9635\n",
            "Epoch 273/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 102.9363\n",
            "Epoch 274/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 102.7429\n",
            "Epoch 275/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 102.5288\n",
            "Epoch 276/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 104.4749\n",
            "Epoch 277/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 107.7686\n",
            "Epoch 278/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 98.6210\n",
            "Epoch 279/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 101.5216\n",
            "Epoch 280/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 100.9108\n",
            "Epoch 281/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 103.8894\n",
            "Epoch 282/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 102.7458\n",
            "Epoch 283/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 100.9965\n",
            "Epoch 284/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 101.9020\n",
            "Epoch 285/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 107.6819\n",
            "Epoch 286/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 103.0002\n",
            "Epoch 287/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 100.4324\n",
            "Epoch 288/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 99.3684\n",
            "Epoch 289/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 103.0010\n",
            "Epoch 290/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 105.5241\n",
            "Epoch 291/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 100.1529\n",
            "Epoch 292/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 101.0777\n",
            "Epoch 293/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 101.0320\n",
            "Epoch 294/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 100.2978\n",
            "Epoch 295/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 97.7150\n",
            "Epoch 296/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 103.8869\n",
            "Epoch 297/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 129.0589\n",
            "Epoch 298/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 101.0329\n",
            "Epoch 299/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 101.6886\n",
            "Epoch 300/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 100.0768\n",
            "Epoch 301/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 107.5757\n",
            "Epoch 302/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 106.3680\n",
            "Epoch 303/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 105.2853\n",
            "Epoch 304/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 102.0329\n",
            "Epoch 305/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 106.3498\n",
            "Epoch 306/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 104.7783\n",
            "Epoch 307/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 103.6096\n",
            "Epoch 308/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - loss: 102.5094\n",
            "Epoch 309/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - loss: 105.3511\n",
            "Epoch 310/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - loss: 104.1527\n",
            "Epoch 311/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - loss: 102.8622\n",
            "Epoch 312/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - loss: 105.1755\n",
            "Epoch 313/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - loss: 103.2052\n",
            "Epoch 314/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - loss: 103.9745\n",
            "Epoch 315/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - loss: 105.3535\n",
            "Epoch 316/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 106.0475\n",
            "Epoch 317/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - loss: 101.2159\n",
            "Epoch 318/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - loss: 102.0712\n",
            "Epoch 319/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - loss: 107.9598\n",
            "Epoch 320/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 102.3259\n",
            "Epoch 321/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 103.7351\n",
            "Epoch 322/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 107.1227\n",
            "Epoch 323/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 98.8453\n",
            "Epoch 324/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 103.9839\n",
            "Epoch 325/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 98.1840\n",
            "Epoch 326/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 102.0569\n",
            "Epoch 327/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 109.1361\n",
            "Epoch 328/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 100.1272\n",
            "Epoch 329/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 99.3978\n",
            "Epoch 330/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 98.0339\n",
            "Epoch 331/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 102.4703\n",
            "Epoch 332/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 99.2262\n",
            "Epoch 333/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 102.4272\n",
            "Epoch 334/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 98.3501\n",
            "Epoch 335/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 99.7517\n",
            "Epoch 336/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 95.7446\n",
            "Epoch 337/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 98.5441\n",
            "Epoch 338/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 101.0864\n",
            "Epoch 339/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 101.5058\n",
            "Epoch 340/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 110.2801\n",
            "Epoch 341/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 96.4925\n",
            "Epoch 342/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 99.5967\n",
            "Epoch 343/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 96.5208\n",
            "Epoch 344/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 98.1577\n",
            "Epoch 345/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 96.4398\n",
            "Epoch 346/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 99.3610\n",
            "Epoch 347/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 99.7126\n",
            "Epoch 348/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 102.0860\n",
            "Epoch 349/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 95.6742\n",
            "Epoch 350/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 98.7271\n",
            "Epoch 351/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 112.3602\n",
            "Epoch 352/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 100.1983\n",
            "Epoch 353/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 103.7014\n",
            "Epoch 354/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 99.7834\n",
            "Epoch 355/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 93.4746\n",
            "Epoch 356/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 98.9193\n",
            "Epoch 357/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 97.9977\n",
            "Epoch 358/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 100.3250\n",
            "Epoch 359/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 97.9675\n",
            "Epoch 360/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 101.2202\n",
            "Epoch 361/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 96.2282\n",
            "Epoch 362/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 102.0214\n",
            "Epoch 363/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 100.0019\n",
            "Epoch 364/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 95.9514\n",
            "Epoch 365/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 96.7540\n",
            "Epoch 366/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 97.3100\n",
            "Epoch 367/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 94.2603\n",
            "Epoch 368/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 98.0296\n",
            "Epoch 369/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 95.3913\n",
            "Epoch 370/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 97.6150\n",
            "Epoch 371/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 96.9163\n",
            "Epoch 372/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 99.7699\n",
            "Epoch 373/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 98.2325\n",
            "Epoch 374/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 98.6258\n",
            "Epoch 375/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 98.2110\n",
            "Epoch 376/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 93.9567\n",
            "Epoch 377/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 94.2511\n",
            "Epoch 378/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 97.5857\n",
            "Epoch 379/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 95.4291\n",
            "Epoch 380/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 91.7302\n",
            "Epoch 381/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 92.6567\n",
            "Epoch 382/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 94.6038\n",
            "Epoch 383/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 91.6412\n",
            "Epoch 384/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 90.5590\n",
            "Epoch 385/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 89.1397\n",
            "Epoch 386/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 94.9906\n",
            "Epoch 387/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 90.7698\n",
            "Epoch 388/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 93.8988\n",
            "Epoch 389/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 91.6849\n",
            "Epoch 390/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 89.6874\n",
            "Epoch 391/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 90.6201\n",
            "Epoch 392/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 87.9542\n",
            "Epoch 393/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 91.8022\n",
            "Epoch 394/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - loss: 94.2289\n",
            "Epoch 395/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - loss: 91.4608\n",
            "Epoch 396/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 89.1069\n",
            "Epoch 397/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 90.0723\n",
            "Epoch 398/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 99.4918\n",
            "Epoch 399/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 93.4529\n",
            "Epoch 400/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - loss: 90.2641\n",
            "Epoch 401/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 92.5983\n",
            "Epoch 402/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step - loss: 99.7865\n",
            "Epoch 403/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - loss: 94.9691\n",
            "Epoch 404/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - loss: 92.6762\n",
            "Epoch 405/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - loss: 95.9067\n",
            "Epoch 406/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 94.7105\n",
            "Epoch 407/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step - loss: 97.2453\n",
            "Epoch 408/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - loss: 92.2477\n",
            "Epoch 409/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 91.6781\n",
            "Epoch 410/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - loss: 93.3036\n",
            "Epoch 411/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 87.3501\n",
            "Epoch 412/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - loss: 91.5271\n",
            "Epoch 413/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - loss: 92.8846\n",
            "Epoch 414/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 89.8562\n",
            "Epoch 415/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 92.5261\n",
            "Epoch 416/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 91.4522\n",
            "Epoch 417/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 89.2969\n",
            "Epoch 418/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 88.7347\n",
            "Epoch 419/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 91.0416\n",
            "Epoch 420/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 90.7885\n",
            "Epoch 421/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 89.7111\n",
            "Epoch 422/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 88.8275\n",
            "Epoch 423/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 95.5288\n",
            "Epoch 424/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 90.5592\n",
            "Epoch 425/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 96.1362\n",
            "Epoch 426/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 95.1511\n",
            "Epoch 427/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 90.4290\n",
            "Epoch 428/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 91.9177\n",
            "Epoch 429/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 92.1003\n",
            "Epoch 430/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 90.8279\n",
            "Epoch 431/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 93.7139\n",
            "Epoch 432/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 89.3592\n",
            "Epoch 433/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 96.7156\n",
            "Epoch 434/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 87.0139\n",
            "Epoch 435/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 87.6772\n",
            "Epoch 436/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 88.0505\n",
            "Epoch 437/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 93.4840\n",
            "Epoch 438/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 89.4931\n",
            "Epoch 439/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 89.8761\n",
            "Epoch 440/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 92.0267\n",
            "Epoch 441/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 88.4031\n",
            "Epoch 442/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 90.0954\n",
            "Epoch 443/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 91.7857\n",
            "Epoch 444/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 97.6013\n",
            "Epoch 445/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 98.8811\n",
            "Epoch 446/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 96.9798\n",
            "Epoch 447/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 103.4003\n",
            "Epoch 448/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 105.2561\n",
            "Epoch 449/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - loss: 101.1490\n",
            "Epoch 450/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 102.0773\n",
            "Epoch 451/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 96.1135\n",
            "Epoch 452/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 101.2600\n",
            "Epoch 453/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 107.0461\n",
            "Epoch 454/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 107.2203\n",
            "Epoch 455/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 105.0428\n",
            "Epoch 456/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 101.3903\n",
            "Epoch 457/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 99.7290\n",
            "Epoch 458/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 101.7946\n",
            "Epoch 459/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 114.1896\n",
            "Epoch 460/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 100.9942\n",
            "Epoch 461/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 101.3889\n",
            "Epoch 462/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 105.6075\n",
            "Epoch 463/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 103.3541\n",
            "Epoch 464/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 99.4854\n",
            "Epoch 465/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 106.4102\n",
            "Epoch 466/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 101.3055\n",
            "Epoch 467/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 103.9849\n",
            "Epoch 468/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 110.4949\n",
            "Epoch 469/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 105.3096\n",
            "Epoch 470/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 99.9719\n",
            "Epoch 471/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 102.7489\n",
            "Epoch 472/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 106.5036\n",
            "Epoch 473/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 102.8811\n",
            "Epoch 474/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 106.2070\n",
            "Epoch 475/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 104.9478\n",
            "Epoch 476/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 100.2912\n",
            "Epoch 477/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 105.6347\n",
            "Epoch 478/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 103.5395\n",
            "Epoch 479/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 99.0785\n",
            "Epoch 480/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 110.8401\n",
            "Epoch 481/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 100.7129\n",
            "Epoch 482/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 101.5743\n",
            "Epoch 483/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 100.9898\n",
            "Epoch 484/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 100.6961\n",
            "Epoch 485/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 105.6174\n",
            "Epoch 486/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 99.3858\n",
            "Epoch 487/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 99.4238\n",
            "Epoch 488/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 100.2545\n",
            "Epoch 489/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - loss: 104.2243\n",
            "Epoch 490/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 103.7280\n",
            "Epoch 491/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - loss: 101.6722\n",
            "Epoch 492/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - loss: 101.1030\n",
            "Epoch 493/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - loss: 107.8862\n",
            "Epoch 494/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - loss: 101.0776\n",
            "Epoch 495/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - loss: 99.1491\n",
            "Epoch 496/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - loss: 103.3833\n",
            "Epoch 497/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - loss: 98.9585\n",
            "Epoch 498/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - loss: 98.5419\n",
            "Epoch 499/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 94.9373\n",
            "Epoch 500/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - loss: 100.4981\n",
            "Epoch 501/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 100.7037\n",
            "Epoch 502/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 95.9149\n",
            "Epoch 503/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 100.4800\n",
            "Epoch 504/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 97.3577\n",
            "Epoch 505/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 98.1478\n",
            "Epoch 506/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 96.7466\n",
            "Epoch 507/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 96.9342\n",
            "Epoch 508/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 96.2076\n",
            "Epoch 509/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 95.7453\n",
            "Epoch 510/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 95.1214\n",
            "Epoch 511/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 96.6651\n",
            "Epoch 512/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 98.7229\n",
            "Epoch 513/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 94.3433\n",
            "Epoch 514/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 101.9743\n",
            "Epoch 515/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 94.3671\n",
            "Epoch 516/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 97.4323\n",
            "Epoch 517/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 102.3120\n",
            "Epoch 518/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 101.6186\n",
            "Epoch 519/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 101.4075\n",
            "Epoch 520/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 97.5595\n",
            "Epoch 521/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 98.7946\n",
            "Epoch 522/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 95.5222\n",
            "Epoch 523/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 91.9704\n",
            "Epoch 524/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 98.0629\n",
            "Epoch 525/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 94.7724\n",
            "Epoch 526/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 97.8526\n",
            "Epoch 527/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 95.3057\n",
            "Epoch 528/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 92.8047\n",
            "Epoch 529/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 91.1875\n",
            "Epoch 530/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 91.4300\n",
            "Epoch 531/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 92.2334\n",
            "Epoch 532/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 93.4991\n",
            "Epoch 533/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 92.3854\n",
            "Epoch 534/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 90.4261\n",
            "Epoch 535/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 88.4946\n",
            "Epoch 536/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 92.7608\n",
            "Epoch 537/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 93.6075\n",
            "Epoch 538/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 88.6897\n",
            "Epoch 539/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 98.6106\n",
            "Epoch 540/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 90.1355\n",
            "Epoch 541/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 92.9527\n",
            "Epoch 542/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 94.4024\n",
            "Epoch 543/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 94.2441\n",
            "Epoch 544/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 88.9804\n",
            "Epoch 545/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 92.8590\n",
            "Epoch 546/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 95.6601\n",
            "Epoch 547/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 87.9554\n",
            "Epoch 548/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 94.9492\n",
            "Epoch 549/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 94.0008\n",
            "Epoch 550/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 93.3705\n",
            "Epoch 551/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 94.9701\n",
            "Epoch 552/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 94.3338\n",
            "Epoch 553/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 97.6154\n",
            "Epoch 554/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 94.9142\n",
            "Epoch 555/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 90.8191\n",
            "Epoch 556/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 92.1146\n",
            "Epoch 557/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 88.2652\n",
            "Epoch 558/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 91.3842\n",
            "Epoch 559/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 87.0316\n",
            "Epoch 560/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 94.8420\n",
            "Epoch 561/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 88.9828\n",
            "Epoch 562/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 87.9415\n",
            "Epoch 563/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 90.6644\n",
            "Epoch 564/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 119.6611\n",
            "Epoch 565/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 91.3886\n",
            "Epoch 566/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 93.5278\n",
            "Epoch 567/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 96.6679\n",
            "Epoch 568/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 93.0631\n",
            "Epoch 569/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 95.2788\n",
            "Epoch 570/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 96.1449\n",
            "Epoch 571/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 94.6541\n",
            "Epoch 572/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - loss: 103.6109\n",
            "Epoch 573/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step - loss: 97.4463\n",
            "Epoch 574/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step - loss: 96.5900\n",
            "Epoch 575/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - loss: 93.7471\n",
            "Epoch 576/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 107.4724\n",
            "Epoch 577/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - loss: 97.9714\n",
            "Epoch 578/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 99.4619\n",
            "Epoch 579/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - loss: 98.2923\n",
            "Epoch 580/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - loss: 100.7697\n",
            "Epoch 581/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - loss: 94.2425\n",
            "Epoch 582/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 98.9500\n",
            "Epoch 583/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 99.8538\n",
            "Epoch 584/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - loss: 96.1087\n",
            "Epoch 585/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 94.4484\n",
            "Epoch 586/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - loss: 96.9307\n",
            "Epoch 587/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - loss: 104.6937\n",
            "Epoch 588/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - loss: 99.4914\n",
            "Epoch 589/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - loss: 97.0610\n",
            "Epoch 590/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 95.4194\n",
            "Epoch 591/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 96.5148\n",
            "Epoch 592/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - loss: 96.9766\n",
            "Epoch 593/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 92.7425\n",
            "Epoch 594/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 89.9435\n",
            "Epoch 595/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 95.1842\n",
            "Epoch 596/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 93.5222\n",
            "Epoch 597/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 90.8867\n",
            "Epoch 598/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 91.6491\n",
            "Epoch 599/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 96.9221\n",
            "Epoch 600/600\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 91.7024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "We use our model to generate new valid molecules from different sampled points of the latent space. We then use RDKit to calculate the qed of the generated smiles. Invalid smiles and any smiles that score below the mean QED threshold for approved drugs (0.492) are rejected and printed to the screen. Generated smiles that score >=0.492 QED are retained and their logP, sas, and molecular weight are calculated using additional RDKit functions. The retained smiles are then displayed in an image. Button and textboxt widgets are displayed that provide the user the option to save the image file and the csv.\n",
        "\n",
        "The user may re-run this code block multiple times to sample and display molecules generated by the VAE. Clicking the Save_CSV after re-run of this code block will then check if there is an existing generated_molecules.csv file (or whatever file name as specified by the user). If there is an existing file, the script will check if the newly generated smiles are duplicates before saving to the CSV. Only new smiles molecules will be saved to the csv. This procedure can be used to append multiple smiles generated by each run of this code block to a csv without adding duplicates.\n",
        "\n",
        "\n",
        "**Reference for QED mean threshold:**\n",
        "Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S., & Hopkins, A. L. (2012). Quantifying the chemical beauty of drugs. *Nature chemistry, 4*(2), 90-98. (https://doi.org/10.1038/nchem.1243)"
      ],
      "metadata": {
        "id": "YuoO28geiUVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_and_save_properties_from_smiles(smiles_list, filename=\"generated_molecules.csv\", print_messages=True):\n",
        "    \"\"\"\n",
        "    Calculates properties for a list of SMILES strings and saves SMILES and QED to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        smiles_list: A list of SMILES strings.\n",
        "        filename: The name of the CSV file to save the data to.\n",
        "    \"\"\"\n",
        "    data_to_save = []\n",
        "    valid_molecules = []\n",
        "    existing_smiles = set() # To store existing SMILES from the csv\n",
        "\n",
        "    # Load existing SMILES from the CSV file if it exists\n",
        "    try:\n",
        "        with open(filename, 'r', newline='') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            next(reader)  # Skip header row\n",
        "            for row in reader:\n",
        "                existing_smiles.add(row[0])  # Add SMILES from the first column\n",
        "    except FileNotFoundError:\n",
        "        pass  # If file doesn't exist, continue\n",
        "\n",
        "    for smiles in smiles_list:\n",
        "        if smiles not in existing_smiles: # Check for duplicates\n",
        "          mol = Chem.MolFromSmiles(smiles)\n",
        "          if mol is not None:\n",
        "            try:\n",
        "                qed_value = QED.qed(mol)  # Calculate QED\n",
        "                if qed_value >= 0.492: # Check if QED is greater than or equal to 0.492\n",
        "                  logp_value = Crippen.MolLogP(mol)  # Calculate logP\n",
        "                  sas_value = sascorer.calculateScore(mol)  # Calculate SAS\n",
        "                  molwt_value = Descriptors.MolWt(mol)  # Calculate molecular weight\n",
        "                  data_to_save.append([smiles, qed_value, logp_value, sas_value, molwt_value])\n",
        "                  valid_molecules.append(mol) # Add to valid molecules list\n",
        "                  existing_smiles.add(smiles) # Add to the set if it was new and the mol was successfully created\n",
        "                else:\n",
        "                  if print_messages:\n",
        "                    print(f\"Molecules with low QED score encountered: {smiles}. Score: {qed_value}\")\n",
        "            except:\n",
        "              if print_messages:\n",
        "                print(f\"Error calculating properties for SMILES: {smiles}\")\n",
        "        else:\n",
        "          if print_messages:\n",
        "            print(f\"Invalid SMILES encountered: {smiles}\")\n",
        "\n",
        "    with open(filename, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        if csvfile.tell() == 0:\n",
        "          writer.writerow([\"smiles\", \"qed\", \"logP\", \"sas\", \"MolWt\"])  # Write header if file is empty\n",
        "        writer.writerows(data_to_save)  # Write new data rows\n",
        "\n",
        "    # Return valid molecules for image generation\n",
        "    return valid_molecules\n",
        "\n",
        "def save_csv(b):\n",
        "    filename = csv_filename_widget.value\n",
        "    calculate_and_save_properties_from_smiles(legends, filename=filename, print_messages=False)  # Pass filename to the function\n",
        "    print(f\"SMILES and properties saved to {filename}\")\n",
        "\n",
        "def save_image(b):\n",
        "    img_filename = filename_widget.value\n",
        "    img_to_save = Image.open(io.BytesIO(pil_img.data))\n",
        "    img_to_save.save(img_filename) # call 'save' on PIL Image, this will overwrite the existing file if it has the same name.\n",
        "\n",
        "    print(f\"Image saved as {img_filename}\")\n",
        "\n",
        "molecules = model.inference(1000)\n",
        "\n",
        "# Generate SMILES legends for valid molecules\n",
        "legends = [Chem.MolToSmiles(m) for m in molecules if m is not None]\n",
        "\n",
        "# Filter molecules based on QED score before creating image\n",
        "valid_molecules_for_image = calculate_and_save_properties_from_smiles(legends, filename=csv_filename_widget.value, print_messages=True)\n",
        "pil_img = MolsToGridImage(\n",
        "    valid_molecules_for_image, molsPerRow=5, subImgSize=(160, 80), legends=legends\n",
        ")\n",
        "\n",
        "display(pil_img)\n",
        "\n",
        "# Create a text box widget for filename input\n",
        "filename_widget = widgets.Text(value=\"my_molecules.png\", description=\"Filename:\")\n",
        "csv_filename_widget = widgets.Text(value=\"generated_molecules.csv\", description=\"CSV Filename:\")\n",
        "\n",
        "display(filename_widget)\n",
        "# Create a button to trigger saving\n",
        "save_button = widgets.Button(description=\"Save Image\")\n",
        "save_button.on_click(save_image)\n",
        "display(save_button)\n",
        "\n",
        "display(csv_filename_widget)\n",
        "# Create a button to trigger saving the CSV\n",
        "save_csv_button = widgets.Button(description=\"Save CSV\")\n",
        "save_csv_button.on_click(save_csv)\n",
        "display(save_csv_button)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4fe3bb4fc97e4f7e93f111952cb0c69e",
            "dd018b4d83b74e2b964bb7923eef938a",
            "c60616561d834385916e1b513e9467cd",
            "70310d53d8f540529ac14d402f712efd",
            "ef4003feb9e5472f9d7ca73ea708672c",
            "71806e09eb2247c4a3f687cf902ad683",
            "ef76a39566354d4ebe25095fdf2157ca",
            "383d5dbf3e4149d0b51d78ee2c7d5ea5",
            "11772c1b11b848c5a17aafdba989b56a",
            "a1687d823ca34c58966b5264a20677de",
            "7028a3e101e64cd7a451ec1a5e2984a7",
            "cf2a6c01d9584faba7d3d1968d6d8678"
          ]
        },
        "id": "P15icAlUbQ8R",
        "outputId": "94b92bd3-8aac-4149-81d1-09abf614b41a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
            "Invalid SMILES encountered: CCCCC1CCCCC1\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC(C2)C3. Score: 0.44677422675976775\n",
            "Invalid SMILES encountered: CCCC1CCCCC1\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC(C2)C3. Score: 0.44677422675976775\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CC1CCC2CCCC1C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: C1C2CC3CC1CC(C2)C3. Score: 0.48010090237684666\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2C31. Score: 0.4432279153146454\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2C31. Score: 0.4432279153146454\n",
            "Molecules with low QED score encountered: C1CC2CC3CC1CC2C3. Score: 0.48010090237684666\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC(C2)C3. Score: 0.44677422675976775\n",
            "Invalid SMILES encountered: CCC1C=COCC1\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: C1C2CC3CC1CC(C2)C3. Score: 0.48010090237684666\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: CCC1C=C2CC(C2)O1. Score: 0.48486054362655656\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C1CC2CC3CC1CC2C3. Score: 0.48010090237684666\n",
            "Molecules with low QED score encountered: CC.CCCCCCCCCCC. Score: 0.43507707798969397\n",
            "Molecules with low QED score encountered: C1C2CC3CC1CC(C2)C3. Score: 0.48010090237684666\n",
            "Molecules with low QED score encountered: CC1=CC(C)CCC1. Score: 0.4204986930836395\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2C31. Score: 0.4432279153146454\n",
            "Molecules with low QED score encountered: CC.CCCCCCCCC. Score: 0.4877898456070398\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Invalid SMILES encountered: CCCC1CCCCC1\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CC.CCCCCCCCCCC. Score: 0.43507707798969397\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Invalid SMILES encountered: CCCCC1CCCCC1\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2C31. Score: 0.4432279153146454\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2C31. Score: 0.4432279153146454\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C.CCCCCCCCC. Score: 0.4848273184328835\n",
            "Molecules with low QED score encountered: C#CC.CCCCCCCCCCC. Score: 0.37063179940222596\n",
            "Molecules with low QED score encountered: CC.CCCCCCCCCCC. Score: 0.43507707798969397\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2C31. Score: 0.4432279153146454\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2=C31. Score: 0.47163156667384004\n",
            "Invalid SMILES encountered: CCCC1CCCCC1\n",
            "Molecules with low QED score encountered: C1=C2CCCC1CCC2. Score: 0.43326108262619045\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC(C2)C3. Score: 0.44677422675976775\n",
            "Invalid SMILES encountered: CCCC1CCCCC1\n",
            "Invalid SMILES encountered: CCCC1CCCCC1\n",
            "Molecules with low QED score encountered: C1C2CC3CC1CC(C2)C3. Score: 0.48010090237684666\n",
            "Molecules with low QED score encountered: C1=C2CCCC1CCC2. Score: 0.43326108262619045\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2C31. Score: 0.4432279153146454\n",
            "Molecules with low QED score encountered: C1=C2CCCC1CCC2. Score: 0.43326108262619045\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: CC1CC2=CC(CCC2)C1. Score: 0.44867915275670994\n",
            "Molecules with low QED score encountered: CC.CCCCCCCCC. Score: 0.4877898456070398\n",
            "Molecules with low QED score encountered: CC.CCCCCCCCC. Score: 0.4877898456070398\n",
            "Molecules with low QED score encountered: CC1CC2=CC(CCC2)C1. Score: 0.44867915275670994\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC(C2)C3. Score: 0.44677422675976775\n",
            "Molecules with low QED score encountered: CC.CC.CCCCCCCCC. Score: 0.46140729925626317\n",
            "Invalid SMILES encountered: CCCC1CCCCC1\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CC.CC.CCCCCCCCC. Score: 0.46140729925626317\n",
            "Molecules with low QED score encountered: CC1=CCC2=COOC1=C2. Score: 0.4744070288755168\n",
            "Molecules with low QED score encountered: CC.CC.CCCCCCCCC. Score: 0.46140729925626317\n",
            "Molecules with low QED score encountered: C1CC2CC3CC1CC2C3. Score: 0.48010090237684666\n",
            "Invalid SMILES encountered: CCCC1CCCCC1\n",
            "Molecules with low QED score encountered: CC.CCCCCCCCCCC. Score: 0.43507707798969397\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2C31. Score: 0.4432279153146454\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC(C2)C3. Score: 0.44677422675976775\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CCC2C3. Score: 0.44677422675976775\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Invalid SMILES encountered: CCCCC1CCCCC1\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CC1CC2=CC(CCC2)C1. Score: 0.44867915275670994\n",
            "Invalid SMILES encountered: CCCC1CCCC(C)C1\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: CC.CCCCCCCCCC. Score: 0.46449083588031437\n",
            "Molecules with low QED score encountered: C1C2CC3CC1CC(C2)C3. Score: 0.48010090237684666\n",
            "Molecules with low QED score encountered: CCCCCCCCC. Score: 0.4777243799437154\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2C31. Score: 0.4432279153146454\n",
            "Molecules with low QED score encountered: C1=C2CCCC1CCC2. Score: 0.43326108262619045\n",
            "Molecules with low QED score encountered: C1C2CC3CC1CC(C2)C3. Score: 0.48010090237684666\n",
            "Molecules with low QED score encountered: C1CC2CC3CC1CC2C3. Score: 0.48010090237684666\n",
            "Molecules with low QED score encountered: CCCCCCCCCC. Score: 0.4684089279615184\n",
            "Molecules with low QED score encountered: C1=C2CCCC1CCC2. Score: 0.43326108262619045\n",
            "Invalid SMILES encountered: CCCC1CCCCC1\n",
            "Invalid SMILES encountered: CCCC1CCCCC1\n",
            "Molecules with low QED score encountered: C1C2CC3CC1CC(C2)C3. Score: 0.48010090237684666\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC1C2C31. Score: 0.4432279153146454\n",
            "Molecules with low QED score encountered: CC.CCCCCCCCC. Score: 0.4877898456070398\n",
            "Molecules with low QED score encountered: CC1CC2CCCC(C1)C2. Score: 0.4818675899459252\n",
            "Invalid SMILES encountered: CCCCC1CCCCC1\n",
            "Molecules with low QED score encountered: C1=C2CC3CC1CC(C2)C3. Score: 0.44677422675976775\n",
            "Molecules with low QED score encountered: C1=C2CCCC1CCC2. Score: 0.43326108262619045\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='my_molecules.png', description='Filename:')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fe3bb4fc97e4f7e93f111952cb0c69e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Save Image', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70310d53d8f540529ac14d402f712efd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='generated_molecules.csv', description='CSV Filename:')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef76a39566354d4ebe25095fdf2157ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Save CSV', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1687d823ca34c58966b5264a20677de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMILES and properties saved to generated_molecules.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display latent space clusters with respect to QED scores"
      ],
      "metadata": {
        "id": "xfnVBfVh_oia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The line of code is necessary the latent space plot in Colab or Jupyter\n",
        "%matplotlib inline\n",
        "\n",
        "# def plot_latent(vae, data, labels): This defines the plot_latent function which takes three arguments:\n",
        "    # vae: The trained VAE model.\n",
        "    # data: The input data used to generate the latent space representation (adjacency and\n",
        "    # feature tensors).\n",
        "    # labels: The property values (QED scores) of the molecules\n",
        "def plot_latent(vae, data, labels):\n",
        "\n",
        "    # display a 2D plot of the property in the latent space\n",
        "    # z_mean, _ = vae.encoder.predict(data): This line uses the encoder part of the VAE\n",
        "     # (vae.encoder) to predict the latent space representation (z_mean) of the input data.\n",
        "     # The underscore _ is used to ignore the second output of the encoder (which is usually\n",
        "     # the variance).\n",
        "    z_mean, _ = vae.encoder.predict(data)\n",
        "    plt.figure(figsize=(12, 10)) # Creates a plot figure with a specified size 12 in by 10 in.\n",
        "\n",
        "    # plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels): This creates the scatter plot:\n",
        "        # z_mean[:, 0]: Represents the values of the first dimension of the latent space.\n",
        "        # z_mean[:, 1]: Represents the values of the second dimension of the latent space.\n",
        "        # c=labels: This assigns colors to the points based on the labels (QED scores).\n",
        "        # Molecules with similar QED scores will have similar colors.\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
        "    plt.colorbar() # This adds a colorbar to the plot to show the mapping between colors and QED scores.\n",
        "\n",
        "    # plt.xlabel(\"z[0]\"), plt.ylabel(\"z[1]\"): These lines label the x and y axes of the plot.\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show() # Displays the created plot\n",
        "\n",
        "# plot_latent(model, [adjacency_tensor[:], feature_tensor[:]], qed_tensor[:]): This line calls\n",
        "# the plot_latent function with:\n",
        "    # model: The trained VAE model.\n",
        "    # [adjacency_tensor[:], feature_tensor[:]]: The adjacency and feature tensors of the molecules,\n",
        "    # used as input to the encoder.\n",
        "    # qed_tensor[:]: The QED scores of the molecules, used to color the points in the scatter plot.\n",
        "plot_latent(model, [adjacency_tensor[:], feature_tensor[:]], qed_tensor[:])"
      ],
      "metadata": {
        "id": "T8j5-fwSJM4E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "a37a9640-1a42-4bac-bdab-bfcc69074169"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Suplemental Code"
      ],
      "metadata": {
        "id": "hGgzWegUR9OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "QED (Quantitative Estimate of Drug-likeness): This score tells us how likely a molecule is to\n",
        "be a good drug candidate. Higher QED scores generally indicate better drug-likeness.\n",
        "SAS (Synthetic Accessibility Score): This score estimates how easy or difficult it is to\n",
        "synthesize a molecule in a laboratory. Lower SAS scores suggest that the molecule is easier to\n",
        "synthesize.\n",
        "LogP (Octanol-Water Partition Coefficient): This value represents the solubility of a molecule\n",
        "in oil (octanol) compared to water. It is an important indicator of how a drug might be\n",
        "absorbed and distributed in the body.\n",
        "'''\n",
        "\n",
        "# Function to calculate QED (Quantitative Estimate of Drug-likeness)\n",
        "def calculate_qed(mol):\n",
        "    try:\n",
        "        return QED.qed(mol)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Function to calculate synthetic accessibility score (SAS)\n",
        "def calculate_sas(mol):\n",
        "  try:\n",
        "    return sascorer.calculateScore(mol)\n",
        "  except:\n",
        "    return None\n",
        "\n",
        "# Function to calculate LogP\n",
        "def calculate_logp(mol):\n",
        "    try:\n",
        "        return Crippen.MolLogP(mol)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Read the CSV file containing SMILES data\n",
        "def process_smiles_data(input_file, output_file):\n",
        "    # Load CSV with SMILES data\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Create columns for QED, SAS, and logP\n",
        "    df['qed'] = None\n",
        "    df['sas'] = None\n",
        "    df['logP'] = None\n",
        "\n",
        "    # Loop through each SMILES and calculate descriptors\n",
        "    for index, row in df.iterrows():\n",
        "        smiles = row['smiles'] # File w/ SMILES has column header titled smiles\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "        if mol:\n",
        "            # Calculate descriptors\n",
        "            df.at[index, 'qed'] = calculate_qed(mol)\n",
        "            df.at[index, 'sas'] = calculate_sas(mol)\n",
        "            df.at[index, 'logP'] = calculate_logp(mol)\n",
        "\n",
        "    # Save the modified dataframe to a new CSV\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "# File path to file with SMILES;\n",
        "csv_path = '/content/drive/My Drive/VAE_GAN_Colab_Notebooks/Data/best_scores_VAE_just SMILES no descriptors.csv' # Change this to your file with smiles\n",
        "# Provide your input and output file names\n",
        "input_file = csv_path  # Your input CSV file\n",
        "output_file = 'smiles_with_descriptors.csv'  # Output CSV file with descriptors\n",
        "\n",
        "# Process the SMILES data and save to the new CSV file\n",
        "process_smiles_data(input_file, output_file)\n",
        "\n",
        "print(f\"Descriptors added and saved to {output_file}\")"
      ],
      "metadata": {
        "id": "2p5pl49Ke27x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}